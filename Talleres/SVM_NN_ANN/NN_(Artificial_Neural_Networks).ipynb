{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P_gebUA5m8IZ",
        "outputId": "b961a64e-b56c-477b-eea0-88b632183e5b"
      },
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "## import the iris dataset for classification\n",
        " \n",
        "from sklearn import datasets\n",
        "iris=sklearn.datasets.load_iris()\n",
        "\n",
        "## print some data, to see the imported dataset\n",
        " \n",
        "print(\"Printing some sample data from the iris dataset\")\n",
        "for training_sample in list(zip(iris.data,iris.target))[:5]:\n",
        "    print(training_sample)\n",
        " \n",
        "## save the features and class\n",
        " \n",
        "features=iris.data   # split iris dataset into features and iris_class\n",
        "print(features)\n",
        "iris_class=iris.target  # class[X] is output corresponding to features[X]\n",
        "print(iris_class)\n",
        "## Split the dataset into training (70%) and testing (30%)\n",
        "## Note that the shuffle parameter has been used in splitting.\n",
        " \n",
        "print(\"Splitting the data into testing and training samples\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "features_train, features_test,iris_class_train, iris_class_test = train_test_split(features,iris_class, test_size=0.33, random_state=42)\n",
        " \n",
        "## data preprocessing: Before training the network we must scale the feature data\n",
        "print(\"Data preprocessing\")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(features_train)\n",
        "features_train_scale = scaler.transform(features_train)\n",
        "features_test_scale = scaler.transform(features_test)\n",
        "features_train\n",
        "features_train_scale\n",
        "\n",
        "## The MLPClassifier and MLPRegressor are sklearn implementations of NNs\n",
        " \n",
        "from sklearn.neural_network import MLPClassifier\n",
        "iterations=1000   # define the iterations for training over the dataset\n",
        "hidden_layers=[10,10,10]  # define the layers/depth of the NN\n",
        " \n",
        "mlp = MLPClassifier(hidden_layer_sizes=(hidden_layers), max_iter=iterations) \n",
        " \n",
        "# an object which represents the neural network\n",
        "# Remember to use the pre-processed data and not original values for fit()\n",
        " \n",
        "mlp.fit(features_train_scale, iris_class_train)  # fit features over NN\n",
        " \n",
        "## Run the test data over the network to see the predicted outcomes.\n",
        " \n",
        "predicted = mlp.predict(features_test_scale)  \n",
        "print(predicted)\n",
        "print(iris_class_test)\n",
        "# predict over test data\n",
        "## evaluation metrics and analysing the accuracy/output.\n",
        "print(\"Evaluation: considering the confusion matrix\")\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(iris_class_test,predicted))  \n",
        "# all non-diagonal elements are 0 if you get 100% accuracy\n",
        "\n",
        "plot_confusion_matrix(mlp, features_test_scale, iris_class_test) \n",
        "plt.show()\n",
        "\n",
        "print(\"Evaluation report:\")\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(iris_class_test,predicted)) \n",
        "#f1-score/accuracy\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score \n",
        "accuracy=accuracy_score(y_true=iris_class_test, y_pred=predicted)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "precision=precision_score(y_true=iris_class_test, y_pred=predicted, average='macro')\n",
        "print('Precision:', precision)\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "recall = recall_score(y_true=iris_class_test, y_pred=predicted, average = 'micro')\n",
        "print('Recall: %f' % recall)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_true=iris_class_test, y_pred=predicted, average= 'micro')\n",
        "print('F1 score: %f' % f1)\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score \n",
        "kappa=cohen_kappa_score (y1= iris_class_test, y2= predicted)\n",
        "print('Cohens kappa: %f' % kappa)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing some sample data from the iris dataset\n",
            "(array([5.1, 3.5, 1.4, 0.2]), 0)\n",
            "(array([4.9, 3. , 1.4, 0.2]), 0)\n",
            "(array([4.7, 3.2, 1.3, 0.2]), 0)\n",
            "(array([4.6, 3.1, 1.5, 0.2]), 0)\n",
            "(array([5. , 3.6, 1.4, 0.2]), 0)\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "Splitting the data into testing and training samples\n",
            "Data preprocessing\n",
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0 0 0 2 1 1 0 0 1 1 2 1 2]\n",
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0 0 0 2 1 1 0 0 1 2 2 1 2]\n",
            "Evaluation: considering the confusion matrix\n",
            "[[19  0  0]\n",
            " [ 0 15  0]\n",
            " [ 0  1 15]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       0.94      1.00      0.97        15\n",
            "           2       1.00      0.94      0.97        16\n",
            "\n",
            "    accuracy                           0.98        50\n",
            "   macro avg       0.98      0.98      0.98        50\n",
            "weighted avg       0.98      0.98      0.98        50\n",
            "\n",
            "Accuracy: 0.980000\n",
            "Precision: 0.9791666666666666\n",
            "Recall: 0.980000\n",
            "F1 score: 0.980000\n",
            "Cohens kappa: 0.969861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d88gen0x9Yvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntXE1owlIdfY",
        "outputId": "98890eba-e3e4-4a1b-afc8-b480527cbbda"
      },
      "source": [
        "print(features_train_scale.shape)\n",
        "print(iris_class_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4)\n",
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYZMBONMKSRR",
        "outputId": "af1c7d3b-36ae-41c1-fa52-d3ad2bc6720e"
      },
      "source": [
        "print(features_train_scale)\n",
        "print(iris_class_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.13835603 -0.26550845  0.22229072  0.10894943]\n",
            " [ 2.14752625 -0.02631165  1.61160773  1.18499319]\n",
            " [-0.25866563 -0.02631165  0.39595535  0.37796037]\n",
            " [-0.8602136   1.16967238 -1.39857913 -1.37061074]\n",
            " [ 2.26783585 -0.50470526  1.66949594  1.05048772]\n",
            " [-0.01804644 -0.74390206  0.16440251 -0.29456698]\n",
            " [-0.739904    0.93047557 -1.39857913 -1.37061074]\n",
            " [-0.98052319  1.16967238 -1.45646733 -1.23610527]\n",
            " [-0.8602136   1.88726279 -1.10913808 -1.1015998 ]\n",
            " [-0.98052319 -2.4182797  -0.18292674 -0.29456698]\n",
            " [ 0.58350153 -0.74390206  0.62750818  0.78147678]\n",
            " [-1.22114238  0.93047557 -1.10913808 -1.37061074]\n",
            " [-0.98052319 -0.02631165 -1.28280271 -1.37061074]\n",
            " [-0.8602136   0.69127877 -1.2249145  -0.96709433]\n",
            " [-0.25866563 -0.74390206  0.22229072  0.10894943]\n",
            " [-0.8602136   0.93047557 -1.34069092 -1.37061074]\n",
            " [-0.13835603 -0.02631165  0.22229072 -0.02555604]\n",
            " [ 2.26783585  1.88726279  1.66949594  1.31949866]\n",
            " [-1.46176157  0.45208196 -1.39857913 -1.37061074]\n",
            " [ 0.46319194 -0.26550845  0.28017893  0.10894943]\n",
            " [-0.13835603 -1.22229567  0.68539639  1.05048772]\n",
            " [-0.37897522  2.84405001 -1.39857913 -1.37061074]\n",
            " [ 0.22257275 -0.02631165  0.56961997  0.78147678]\n",
            " [-0.01804644 -0.74390206  0.7432846   0.91598225]\n",
            " [ 0.22257275 -1.93988609  0.1065143  -0.29456698]\n",
            " [-0.49928482 -0.02631165  0.39595535  0.37796037]\n",
            " [ 0.46319194  0.93047557  0.91694923  1.45400413]\n",
            " [-0.37897522 -1.70068929  0.1065143   0.10894943]\n",
            " [-0.49928482  2.1264596  -1.2249145  -1.1015998 ]\n",
            " [-0.98052319 -1.70068929 -0.29870316 -0.29456698]\n",
            " [ 0.70381112 -0.74390206  0.85906102  0.91598225]\n",
            " [-0.98052319  0.69127877 -1.39857913 -1.37061074]\n",
            " [-0.98052319  0.45208196 -1.51435554 -1.37061074]\n",
            " [-0.37897522 -1.46149248 -0.00926211 -0.16006151]\n",
            " [ 1.06473991 -0.02631165  0.68539639  0.64697131]\n",
            " [-1.10083279  0.21288516 -1.34069092 -1.37061074]\n",
            " [-0.01804644 -0.50470526  0.7432846   1.5885096 ]\n",
            " [-0.98052319  0.93047557 -1.34069092 -1.37061074]\n",
            " [-0.98052319  1.16967238 -1.28280271 -0.83258886]\n",
            " [ 0.10226315  0.45208196  0.56961997  0.78147678]\n",
            " [-0.8602136  -1.22229567 -0.47236778 -0.16006151]\n",
            " [ 1.30535909  0.45208196  1.09061385  1.45400413]\n",
            " [ 0.22257275 -0.74390206  0.7432846   0.51246584]\n",
            " [ 0.34288234 -0.98309887  1.03272565  0.2434549 ]\n",
            " [ 2.26783585 -0.02631165  1.32216669  1.45400413]\n",
            " [-0.37897522 -1.22229567  0.1065143   0.10894943]\n",
            " [-1.70238076 -0.26550845 -1.39857913 -1.37061074]\n",
            " [-1.82269035 -0.02631165 -1.57224375 -1.50511621]\n",
            " [ 0.22257275 -1.93988609  0.68539639  0.37796037]\n",
            " [ 1.66628788  0.45208196  1.26427848  0.78147678]\n",
            " [-1.46176157  0.21288516 -1.34069092 -1.37061074]\n",
            " [-0.8602136   1.16967238 -1.39857913 -1.23610527]\n",
            " [-1.70238076 -0.02631165 -1.45646733 -1.37061074]\n",
            " [ 0.58350153 -1.22229567  0.62750818  0.37796037]\n",
            " [ 0.58350153  0.93047557  1.03272565  1.5885096 ]\n",
            " [-1.46176157  0.93047557 -1.39857913 -1.23610527]\n",
            " [ 1.1850495  -0.02631165  0.97483744  1.18499319]\n",
            " [ 0.58350153  0.69127877  1.26427848  1.72301507]\n",
            " [-1.34145197  0.45208196 -1.45646733 -1.37061074]\n",
            " [ 0.34288234 -0.26550845  0.51173177  0.2434549 ]\n",
            " [ 0.82412072 -0.50470526  0.45384356  0.37796037]\n",
            " [ 0.46319194 -0.50470526  0.56961997  0.78147678]\n",
            " [ 1.42566869  0.45208196  0.51173177  0.2434549 ]\n",
            " [ 0.70381112  0.45208196  0.85906102  1.45400413]\n",
            " [-0.8602136   1.88726279 -1.28280271 -1.37061074]\n",
            " [ 1.30535909  0.21288516  0.91694923  1.18499319]\n",
            " [ 0.10226315 -0.02631165  0.22229072  0.37796037]\n",
            " [ 0.82412072 -0.02631165  0.80117281  1.05048772]\n",
            " [-0.13835603 -0.98309887 -0.18292674 -0.29456698]\n",
            " [-0.739904   -0.74390206  0.0486261   0.2434549 ]\n",
            " [ 0.34288234 -0.02631165  0.45384356  0.2434549 ]\n",
            " [-1.58207116 -1.70068929 -1.45646733 -1.23610527]\n",
            " [ 0.94443031 -0.26550845  0.45384356  0.10894943]\n",
            " [-0.37897522 -0.98309887  0.33806714 -0.02555604]\n",
            " [-0.61959441  1.64806599 -1.34069092 -1.37061074]\n",
            " [-0.25866563 -0.02631165  0.16440251  0.10894943]\n",
            " [ 1.78659747 -0.26550845  1.43794311  0.78147678]\n",
            " [ 1.06473991  0.69127877  1.09061385  1.18499319]\n",
            " [-0.8602136   1.64806599 -1.34069092 -1.1015998 ]\n",
            " [-1.10083279 -1.46149248 -0.29870316 -0.29456698]\n",
            " [ 1.06473991  0.69127877  1.09061385  1.72301507]\n",
            " [ 1.66628788 -0.02631165  1.14850206  0.51246584]\n",
            " [-1.10083279  1.40886918 -1.39857913 -1.50511621]\n",
            " [ 1.06473991  0.21288516  1.03272565  1.5885096 ]\n",
            " [-1.10083279 -0.02631165 -1.39857913 -1.37061074]\n",
            " [ 1.30535909  0.21288516  0.62750818  0.37796037]\n",
            " [ 1.90690706 -0.50470526  1.32216669  0.91598225]\n",
            " [ 0.58350153 -0.26550845  1.03272565  0.78147678]\n",
            " [-0.13835603 -0.50470526  0.16440251  0.10894943]\n",
            " [ 0.82412072 -0.02631165  0.97483744  0.78147678]\n",
            " [ 0.58350153 -1.70068929  0.33806714  0.10894943]\n",
            " [ 0.70381112 -0.26550845  0.28017893  0.10894943]\n",
            " [-0.25866563 -0.50470526  0.62750818  1.05048772]\n",
            " [ 0.10226315 -0.02631165  0.7432846   0.78147678]\n",
            " [-0.49928482  0.93047557 -1.2249145  -1.37061074]\n",
            " [ 0.34288234 -0.50470526  0.1065143   0.10894943]\n",
            " [-1.10083279 -1.22229567  0.39595535  0.64697131]\n",
            " [-0.01804644  2.3656564  -1.51435554 -1.37061074]\n",
            " [-0.01804644 -0.98309887  0.1065143  -0.02555604]\n",
            " [ 1.54597828 -0.02631165  1.20639027  1.18499319]]\n",
            "[1 2 1 0 2 1 0 0 0 1 2 0 0 0 1 0 1 2 0 1 2 0 2 2 1 1 2 1 0 1 2 0 0 1 1 0 2\n",
            " 0 0 1 1 2 1 2 2 1 0 0 2 2 0 0 0 1 2 0 2 2 0 1 1 2 1 2 0 2 1 2 1 1 1 0 1 1\n",
            " 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1 2]\n"
          ]
        }
      ]
    }
  ]
}